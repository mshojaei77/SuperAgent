--- Page 1 ---
Aligning Large Language Models with Human Preferences
through Representation Engineering
Anonymous ACL submission
Abstract
Aligning large language models (LLMs) with
001
human preferences is crucial for enhancing
002
their utility in terms of helpfulness, truthful-
003
ness, safety, harmlessness, and interestingness.
004
Existing methods for achieving this alignment
005
often involve employing reinforcement learn-
006
ing from human feedback (RLHF) to fine-tune
007
LLMs based on human labels assessing the rel-
008
ative quality of model responses. Nevertheless,
009
RLHF is susceptible to instability during fine-
010
tuning and presents challenges in implementa-
011
tion. Drawing inspiration from the emerging
012
field of representation engineering (RepE), this
013
study aims to identify relevant representations
014
for high-level human preferences embedded in
015
patterns of activity within an LLM and achieve
016
precise control of model behavior by transform-
017
ing its representations. This novel approach,
018
denoted as Representation Alignment from Hu-
019
man Feedback (RAHF), proves to be effective,
020
computationally efficient, and easy to imple-
021
ment. Extensive experiments demonstrate the
022
efficacy of RAHF in not only capturing but also
023
manipulating representations to align with a
024
broad spectrum of human preferences or values,
025
rather than being confined to a singular concept
026
or function (e.g. honesty or bias). RAHF’s
027
versatility in accommodating diverse human
028
preferences shows its potential for advancing
029
LLM performance.
030
1
Introduction
031
While large language models (LLMs) learn broad-
032
ranging world knowledge and a degree of reason-
033
ing proficiency, precise control over their behavior
034
proves challenging due to the unsupervised nature
035
of their pre-training (Radford et al., 2018, 2019;
036
Brown et al., 2020; Bubeck et al., 2023; Touvron
037
et al., 2023). For each query, instruction-tuned
038
LLMs (Wei et al., 2021; Chung et al., 2022; Tou-
039
vron et al., 2023) exhibit the capacity to gener-
040
ate multiple responses that are both semantically
041
Figure 1: Illustration of different apporaches. (a) Re-
inforcement learning from human feedback (RLHF);
(b) Direct preference optimization (DPO); (c) Hindsight
instruction relabeling (HIR); (d) Representation align-
ment from human feedback (RAHF).
and syntactically coherent by some sampling tech-
042
niques. While such ability enables the models to
043
provide diversity that is essential for chat agents,
044
some responses may contain harmful, unethical, so-
045
cially biased, and negative, even illegal content (Sri-
046
vastava et al., 2022; Thoppilan et al., 2022; Bubeck
047
et al., 2023; Wang et al., 2023).
048
Existing methods steer LLMs to align with hu-
049
man preferences often using reinforcement learn-
050
ing (RL), with reinforcement learning from human
051
feedback (RLHF) emerging as the most successful
052
one (Ouyang et al., 2022). However, the underlying
053
learning algorithms exhibit a considerable degree
054
of complexity, sensitivity to hyperparameters, in-
055
stability during training, and necessitate additional
056
training in the reward model and value network,
057
leading to substantial computational costs (Yuan
058
et al., 2023; Rafailov et al., 2023).
059
In addressing the aforementioned challenges
060
posed by RL-based methods, several computation-
061
ally lightweight alternatives have been proposed to
062
simplify the human preference-matching process.
063
Two prominent paradigms among these alternatives
064
include contrastive learning (Rafailov et al., 2023;
065
1


Image Path: e:\Codes\super_agent\files\image_1_1.png
Alt Text: Image on page 1
Description: Description for image_1_1.png
Metadata: {}

--- Page 2 ---
Zhao et al., 2023; Yuan et al., 2023) and Hindsight
066
instruction relabeling (HIR) (Zhang et al., 2023;
067
Liu et al., 2023). Contrastive learning-based meth-
068
ods optimize a language model policy by increasing
069
the relative probability of preferred responses over
070
dispreferred ones, while HIR methods transform
071
human feedback into instructions by relabeling the
072
original ones, indicating the relative quality of pro-
073
vided responses. A common characteristic shared
074
by these two paradigms is their capability to align
075
language models with human preferences through
076
reward-free fine-tuning.
077
However, the reward-free fine-tuning is vulner-
078
able to the presence of noisy data or incorrect
079
labels in a training set comprising a collection
080
of preference-annotated response pairs(Li et al.,
081
2023b; Dumoulin et al., 2023). Instances of dull
082
sentences or very brief responses may appear re-
083
peatedly in such a training set, potentially intro-
084
ducing bias into the models. The exclusion of such
085
instances from the training set renders it impossible
086
for LLMs to glean insights into human preferences
087
expressed in these instances. In contrast, RL-based
088
methods adopt a different strategy, wherein a re-
089
ward function is first extracted from a dataset of
090
response rankings, and then this reward function
091
can be applied to train an LLM, effectively miti-
092
gating the model’s direct exposure to noisy data or
093
incorrect labels within the dataset.
094
In this study, we aim to seek for a computa-
095
tionally lighter and reward-free algorithm that can
096
effectively harness human preference expressed
097
in datasets meanwhile safeguarding LLMs from
098
the influence of noisy data. Inspired by the recent
099
advance in representation engineering (Zou et al.,
100
2023), we initially locate relevant representations
101
and activity patterns associated with high-level hu-
102
man preferences within an LLM, and subsequently
103
gain precise control over its behavior by manip-
104
ulating its internal representations. In the neural
105
architecture, network weights determine neural ac-
106
tivity, neural activity determines the networks’ out-
107
put, and the networks’ output determines the net-
108
works’ behavior. Instead of focusing on neurons
109
and their connections, we see aligning LLMs with
110
human feedback as an outcome of representational
111
spaces, implemented by patterns of activity across
112
populations of neurons. We first identify the dif-
113
ferences in model activities between preferred and
114
dispreferred stimuli, and then control its behavior
115
by leveraging the identified differences in represen-
116
tations (see Figure 1). We introduce two methods
117
for controlling representations and demonstrate the
118
efficacy of these representation engineering (RepE)
119
approaches in aligning LLMs with a broad spec-
120
trum of human preferences through a collection of
121
response pairs.
122
To validate the effectiveness of our approach in
123
aligning with human preferences, we conducted ex-
124
tensive comparative experiments on the generated
125
results. Our method outperformed RLHF and other
126
RL-free approaches in human evaluations and auto-
127
mated metrics such as general abilities and GPT-4
128
evaluations. Notably, the underlying algorithms
129
exhibit simplicity in implementation and straight-
130
forwardness in training.
131
2
Related Work
132
Tuning large language models to elicit desired re-
133
sponses and behavior from their extensive knowl-
134
edge and capabilities is essential in the develop-
135
ment of chat agents, such as ChatGPT (Brown et al.,
136
2020), LLaMA (Touvron et al., 2023) and GPT-4
137
(Bubeck et al., 2023), characterized by safety, per-
138
formance, and controllability. The enlargement
139
of the size of language models only does not in-
140
herently enhance their ability to follow a user’s
141
intent. For example, LLMs may still generate out-
142
puts that are untruthful, toxic, or simply not helpful
143
to the user. Existing human preference alignment
144
methods can be broadly classified into three major
145
categories: reinforcement learning (Ouyang et al.,
146
2022; Ramamurthy et al., 2023), contrastive learn-
147
ing (Rafailov et al., 2023; Zhao et al., 2023; Yuan
148
et al., 2023), and Hindsight instruction relabeling
149
(Zhang et al., 2023; Liu et al., 2023).
150
Extensive research has been devoted to the explo-
151
ration of RL from human feedback through ratings
152
or rankings, spanning tasks from NL-to-SQL con-
153
version (Zhong et al., 2017), machine translation
154
(Kreutzer et al., 2018), task-oriented dialogue sys-
155
tems (Su et al., 2019; Zhang et al., 2019; Takanobu
156
et al., 2019), summarization (Stiennon et al., 2020),
157
story-telling (Ziegler et al., 2019) to instruction-
158
following (Ouyang et al., 2022; Ramamurthy et al.,
159
2023). Typically, these methods involve the fitting
160
of a reward model to a dataset of human prefer-
161
ences, followed by the optimization of a LLM pol-
162
icy to generate responses with high reward, using
163
RL algorithms such as REINFORCE (Williams,
164
1992) or proximal policy optimization (Schulman
165
et al., 2017). Despite the attractiveness of lever-
166
aging human preferences that are easier to collect
167
2


--- Page 3 ---
than expert demonstrations, training LLMs with RL
168
poses significant practical challenges, which is at-
169
tributed to the sensitivity of RL to hyperparameters
170
and the inherent instability during training.
171
The solutions based on Hindsight instruction re-
172
labeling (Zhang et al., 2023; Liu et al., 2023) and
173
contrastive learning (Rafailov et al., 2023; Zhao
174
et al., 2023; Yuan et al., 2023) have emerged as
175
computationally efficient alternatives to RL-based
176
methods without explicit reward modeling. How-
177
ever, these reward-free fine-tuning solutions are
178
susceptible to noisy data or incorrect labels within
179
a training set. They exhibit performance lags com-
180
pared to models tuned with RL counterparts (see
181
Section 4). Furthermore, the question of whether
182
LLMs trained with such fine-tuning methods can
183
generalize well to out-of-distribution queries re-
184
mains unresolved when contrasted with models
185
incorporating an explicit reward model. RLHF
186
method (Ouyang et al., 2022) offers a potential
187
avenue for improvement by leveraging additional
188
unlabeled examples through labeling LLM genera-
189
tions with the learned reward model.
190
To enhance transparency and controllability of
191
neural networks, Zou et al. (2023) introduced rep-
192
resentation engineering (RepE) as a methodology,
193
drawing an analogy between understanding deep
194
neural networks through representation tomogra-
195
phy and studying brains via neuroimaging tech-
196
niques. Their work demonstrated the efficacy of
197
RepE in addressing diverse safety-related chal-
198
lenges such as truthfulness, honesty, and hallucina-
199
tion. This study falls in line with recent research
200
findings and extends its application to aligning
201
LLMs with a wide spectrum of human preferences.
202
Our study introduces two novel methods to instruct
203
LLMs on human preferences first, and then extract
204
differences in model activities between preferred
205
and dispreferred stimuli. These differences in activ-
206
ity patterns serve as a foundation for manipulating
207
the model’s behavior, leading to the generation of
208
responses that better align with human preferences.
209
Due to the lightweight computational advantages of
210
parameter-efficient fine-tuning techniques(Houlsby
211
et al., 2019; Lester et al., 2021; Hu et al., 2021;
212
Wu et al., 2023), these techniques are utilized to
213
fit the disparity in activity patterns. In contrast to
214
the approach adopted by Zou et al. (2023), which
215
relies on unlabeled or self-generated stimuli lim-
216
ited to singular concepts or functions the meaning
217
of which the models have already “known”, our
218
methods provide a more comprehensive alignment
219
with diverse human preferences.
220
3
Method
221
We begin by instructing LLMs on human prefer-
222
ences with a set of preference-annotated response
223
pairs. We introduce two novel methods for instruct-
224
ing LLMs on human preferences and extracting
225
their activity patterns: one involving a single LLM
226
(trained to discern the relative quality of responses)
227
and the other employing dual LLMs (“a good guy
228
and a bad guy”). Secondly, we collect the activity
229
patterns of LLMs when exposed to stimuli that are
230
preferred or dispreferred. The differences in these
231
patterns serve as the foundation for manipulating
232
LLMs, enabling them to generate responses more
233
closely aligned with human values. Finally, we
234
construct the final model by training a low-rank
235
adapter(Hu et al., 2021) to fit the disparity in activ-
236
ity patterns.
237
3.1
Instructing LLMs on Human Preferences
238
To extract activity patterns from the model that
239
align with human preferences, it is crucial for
240
the model to possess a correct understanding and
241
awareness of these preferences. The effectiveness
242
of extracting activity patterns from alignment fine-
243
tuned models, such as LLaMA-2-chat, in capturing
244
concepts like truthfulness and honesty has been
245
validated by Zou et al. (2023). However, for non-
246
aligned models, such as pre-trained large language
247
models or LLMs subjected to simple fine-tuning,
248
explicit indications of human preferences should
249
be provided to elicit and capture activity patterns
250
induced by stimulus preferences. This capability
251
enables the accumulation of diverse activities, sub-
252
sequently utilized to calibrate LLMs based on hu-
253
man preferences.
254
For instructing LLMs on human preferences, we
255
rely on a dataset annotated with human preferences.
256
As mentioned earlier, we employ two methods to
257
achieve this goal. The first method utilizes Hind-
258
sight(Zhang et al., 2023), using contrastive instruc-
259
tions to instruct a single LLM. The second method
260
involves fine-tuning two LLMs separately: one (re-
261
ferred to as the preferred model) is fine-tuned based
262
on preferred responses, while the other (referred to
263
as the dispreferred model) is fine-tuned on dispre-
264
ferred responses.
265
3


--- Page 4 ---
You are a good, respectful and 
honest assistant …
How can the Chinese football 
team qualify for the World Cup?
Query
Instruction
To qualify for the World Cup, the 
Chinese football team must …
Response
You are a bad, disrespectful and 
dishonest assistant .…
How can the Chinese football 
team qualify for the World Cup?
Query
Instruction
Just play better football.
Response
RAHF-SCIT
Activity Pattern
How can the Chinese football 
team qualify for the World Cup?
Query
To qualify for the World Cup, the 
Chinese football team must …
Response
How can the Chinese football 
team qualify for the World Cup?
Query
Just play better football.
Response
RAHF-Dual
+
Superposition of Activations
Mean Squared
 
Error Loss
Embedding
Transformer 
Layer
× L
Transformer 
Layer
...
Base Model
Embedding
Transformer 
Layer
× L
Transformer 
Layer
...
Final Model
Embedding
Transformer 
Layer
× L
Transformer 
Layer
...
Preferred Model
Embedding
Transformer 
Layer
× L
Transformer 
Layer
...
Dispreferred Model
Discriminative 
Model
Embedding
Transformer 
Layer
× L
Transformer 
Layer
...
Backpropagation
Figure 2: The procedure of RAHF. RAHF begins with the introduction of two methods to instruct LLMs on human
preferences. One approach involves training a single LLM to discern the relative quality of responses (RAHF-
SCIT), while the other employs dual LLMs to model preferred and dispreferred responses separately (RAHF-Dual).
Specifically, RAHF-SCIT takes preferred and dispreferred instructions along with their corresponding responses
as input and conducts contrastive instruction tuning on a single model. RAHF-Dual, on the other hand, performs
supervised training by taking preferred and dispreferred responses into different models. Subsequently, we obtain
activity patterns by stimulating the model with different instructions. We consider the differences between the two
activity patterns as indicative of preferred signals and leverage these signals to finetune the final model with LoRA.
3.1.1
Preference Instruction with a Single
266
Model
267
Within the proposed framework, the Single LLM
268
Method focuses on fine-tuning a Single Large
269
Language Model through Contrastive Instruction
270
Tuning (SCIT). This process involves two instruc-
271
tions: one instructs the model to generate responses
272
preferred by humans, while the other guides the
273
model to generate responses dispreferred by hu-
274
mans. Following such fine-tuning, we can optimize
275
the model for consistency with human preferences.
276
We can also stimulate the model to elicit distinct ac-
277
tivity patterns by employing different instructions
278
subsequently.
279
Specifically, the training dataset is curated to
280
include pairs of both preferred and dispreferred in-
281
structions, alongside associated queries and their
282
corresponding responses (details on preferred in-
283
structions can be found in Appendix A.1). Follow-
284
ing HIR(Zhang et al., 2023), for instructions linked
285
to positive preferences, the fine-tuning objective
286
aims to increase the probability of generating pre-
287
ferred responses while concurrently decreasing the
288
probability of generating dispreferred responses.
289
Conversely, for instructions associated with neg-
290
ative preferences, the objective is to elevate the
291
probability of generating dispreferred responses
292
and reduce the probability of generating preferred
293
responses.
294
Formally, let D represent the training dataset,
295
with qi denoting the query, ri representing the re-
296
sponse, and pi indicating the instruction (positive
297
or negative). The fine-tuning of the LLM involves
298
minimizing the following loss:
299
L = −
X
(pi,qi,ri)∈D
(P + + log
exp (P +)
exp (P +) + exp (P −))
(1)
300
where P += log π(ri | pi, qi; θ), P −= log π(ri|
301
p∗
i , qi; θ) and p∗
i denotes the opposite instruction,
302
ensuring a contrast between preferred and dispre-
303
ferred cases.
304
Throughout the entire fine-tuning process, the
305
LLM undergoes a learning phase to distinguish
306
between preferred and non-preferred responses, re-
307
vealing distinct activity patterns associated with hu-
308
man preferences. Subsequently, these two instruc-
309
tions will serve as stimuli to acquire the model’s
310
internal representations, which will be used for fur-
311
ther alignment. This contrastive training relying
312
on preference data enables the achievement of the
313
4


Image Path: e:\Codes\super_agent\files\image_4_1.jpeg
Alt Text: Image on page 4
Description: Description for image_4_1.jpeg
Metadata: {}

Image Path: e:\Codes\super_agent\files\image_4_2.jpeg
Alt Text: Image on page 4
Description: Description for image_4_2.jpeg
Metadata: {}

--- Page 5 ---
overarching goal of consistency with a broad spec-
314
trum of human preferences, rather than a singular
315
concept.
316
3.1.2
Preference Instruction with Dual Models
317
In the Dual LLMs method, we aim to train two
318
LLMs with distinct tendencies: one model is in-
319
clined to generate preferred responses, while the
320
other tends to produce dispreferred responses. To
321
achieve this objective, we employ paired prefer-
322
ence data to conduct supervised fine-tuning of the
323
LLMs. Specifically, we use the preferred data from
324
the preference pairs to train the preferred model
325
and the dispreferred data from the preference pairs
326
to train the dispreferred model.
327
Formally, consider the dataset D, which consists
328
of input queries q and corresponding pairs of pref-
329
erential responses: a preferred response rh and a
330
dispreferred response rl. We are now dividing D
331
into a preferred dataset Dh = {q, rh}i and a dispre-
332
ferred dataset Dl = {q, rl}i. Utilizing this data, we
333
employ a supervised learning approach (maximum
334
likelihood) to fine-tune the LLMs, thereby obtain-
335
ing two models expressing preferences, denoted as
336
πh and πl respectively. The fine-tuning of these
337
two LLMs is aimed at maximizing the achievement
338
of the following objectives:
339
πh(θ∗) = arg max
θ
X
(qi,ri)∈Dh
log π(ri | qi; θ) (2)
340
341
πl(θ∗) = arg max
θ
X
(qi,ri)∈Dl
log π(ri | qi; θ) (3)
342
Through this training process, the preferred
343
model and dispreferred model have respectively
344
learned the activity patterns associated with human-
345
preferred and dispreferred responses. Due to the
346
human preference learning conducted in two dis-
347
tinct models, in contrast to SCIT, the Dual LLMs
348
method does not require additional distinct instruc-
349
tions during fine-tuning.
Instead, guidance for
350
the model is provided solely through different re-
351
sponses.
352
3.2
Collecting Activity Patterns
353
Following the establishment of comprehension of
354
human preferences by LLMs, we are able to ex-
355
tract representations of what humans prefer and
356
disprefer. Due to the characteristics of autoregres-
357
sive Transformer language models, the attention
358
mechanism results in tokens at different positions
359
exhibiting distinct representations. The activation
360
representation of a token at the current position is
361
influenced by preceding tokens. Therefore, for a
362
specific pair of query q and response r, this pair is
363
concatenated with two instructions from Section
364
3.1, which guide the model in forming the concept
365
of human preferences and inputted into the model
366
to obtain the intermediate layer hidden states at
367
each position as internal representations.
368
Formally, for a given instruction p, a decoder
369
model π, and a function R that processes model
370
input to return the positional representation of re-
371
sponse tokens, we collect the l-th layer’s hidden
372
states of each token for the query-response pair
373
(qi, ri) within dataset D. This is formalized as fol-
374
lows:
375
Ap,π,l = R(π, (p, qi, ri), l) | (qi, ri) ∈D
(4)
376
Moreover, we define a difference vector, which
377
is represented as:
378
vl = Ap+,π,l −Ap−,π,l
(5)
379
This difference vector vl represents the difference
380
in activation patterns produced under the two differ-
381
ent stimulus conditions p+ and p−. Subsequently,
382
we perturb the model’s original representation by
383
incorporating the difference vectors. This pertur-
384
bation serves to guide the model’s representation
385
in the direction aligned with human preferences.
386
It is noteworthy that, for the Single Large Lan-
387
guage Model through Contrastive Instruction Tun-
388
ing (SCIT), both Ap+,π,l and Ap−,π,l are generated
389
by the same model. In the dual LLMs approach,
390
pairs concatenated with different instructions are
391
inputted into the respective preferred and dispre-
392
ferred models, thereby enabling the independent
393
extraction of activation patterns from each model.
394
3.3
Constructing Final Models
395
In this phase, we construct the final model by lever-
396
aging the difference vector vl, derived in Section
397
3.2 to perturb the original representations.
To
398
achieve this, we draw inspiration from the ap-
399
proach of (Zou et al., 2023) by employing a special-
400
ized loss function and fine-tuning with Low-Rank
401
Adapters (LoRA), enabling the efficient incorpora-
402
tion of activation patterns into the model.
403
We consider the output of the LoRA matrix as
404
a perturbation of the original hidden layer states,
405
aligning it with the difference vector. Specifically,
406
we employ Mean Squared Error (MSE) loss as the
407
objective function:
408
LAlign = ∥(Ap,πLoRA,l −(Ap,πbase,l + αvl))∥2
(6)
409
5


--- Page 6 ---
Here, α serves as a hyperparameter controlling the
410
extent to which the difference vector vl intervenes
411
in the model integration process. Ap,πLoRA,l and
412
Ap,πbase,l represent the activity patterns of the tar-
413
get model equipped with and without LoRA, re-
414
spectively. vl is the extracted difference vector as
415
outlined in Section 3.2. In the case of SCIT, vl re-
416
sults from contrasting activity patterns induced by
417
stimulus pairs input to the “discriminative” model,
418
while for the Dual LLM Method, it is obtained by
419
contrasting patterns resulting from inputting stim-
420
ulus pairs fed into the models playing “good guy”
421
and “bad guy” respectively.
422
4
Experiment
423
Following Rafailov et al. (2023), we mainly con-
424
ducted experiments on single-turn dialogue tasks.
425
We extensively compared various RL-free align-
426
ment approaches and RLHF, evaluating the results
427
through human evaluation and automated assess-
428
ment. Additionally, we conducted comparative
429
experiments with the representation engineering
430
method proposed by Zou et al. (2023), serving as
431
an ablation study to demonstrate the impact of our
432
approach in capturing human preferences.
433
4.1
Experimental Setups
434
Dataset
In single-turn dialogue, we use Ultra-
435
Feedback dataset1 (Cui et al., 2023), denoting hu-
436
man preference responses. Each example in the
437
dataset contains a pair of dialogues between a hu-
438
man and a language model, providing preferred
439
and dispreferred responses for each query.
440
Base Model
Ouyang et al. (2022) and Rama-
441
murthy et al. (2023) utilized supervised fine-tuning
442
models as initial models in their application of
443
Proximal Policy Optimization (PPO). For a fair
444
comparison, we performed fine-tuning on the
445
LLaMA2-7B model (Touvron et al., 2023) using
446
Anthropic’s Helpful and Harmless dataset2 (Bai
447
et al., 2022). We denote the resulting model after
448
fine-tuning as the Base Model. In our experiments,
449
all the models were initialized with this model and
450
further trained by the baseline methods and RAHF.
451
4.2
Baselines
452
To evaluate our proposed approach, we conduct ex-
453
tensive comparisons with existing alignment meth-
454
1https://huggingface.co/datasets/argilla/
ultrafeedback-binarized-preferences-cleaned
2https://huggingface.co/datasets/Dahoas/
full-hh-rlhf
ods, including Reinforcement Learning from Hu-
455
man Feedback (RLHF) and other alternative meth-
456
ods for preference alignment. These experiments
457
were specifically designed to assess the efficacy of
458
our method in aligning with human preferences.
459
Preferred-SFT
This baseline involves fine-
460
tuning the language model directly using the pre-
461
ferred responses from the dataset. The model is
462
trained to generate responses that align with the
463
labeled preferred responses.
464
HIR
Hindsight Instruction Relabeling (HIR) pro-
465
posed by Zhang et al. (2023) converts feedback to
466
instruction by relabeling original instructions and
467
employs supervised training for enhanced align-
468
ment with human preferences. We use HIR as a
469
baseline to evaluate the advantages of RAHF over
470
supervised fine-tuning.
471
DPO
Direct Preference Optimization (Rafailov
472
et al., 2023) directly optimizes a language model to
473
adhere to human preferences without using explicit
474
reward modeling or reinforcement learning. It has
475
been proven to be an efficient and straightforward
476
alternative to RLHF.
477
RLHF-PPO
For the RLHF baseline, we follow
478
the common practice, as outlined by Ouyang et al.
479
(2022). We use human preference data to train a
480
reward model and then employ Proximal Policy Op-
481
timization (PPO) to optimize the model generated
482
by supervised fine-tuning.
483
Further elaboration and details regarding the im-
484
plementation of the baseline and our methods are
485
provided in Appendix B.
486
4.3
Automatic Evaluation
487
To validate the effectiveness of our proposed
488
method in aligning models with human prefer-
489
ences, automated evaluations were carried out on
490
models trained via RAHF and various baseline
491
methodologies, focusing on their general capabil-
492
ities and the quality of generation. Specifically,
493
we assessed the performance of different models
494
across three widely used benchmarks: Open LLM
495
Leaderboard(Beeching et al., 2023), AlpacaEval(Li
496
et al., 2023a), and MT-Bench(Zheng et al., 2023).
497
In Appendiex B.2, we detail the evaluation setting
498
adopted by both the leaderboard and our experi-
499
ments.
500
4.3.1
Evaluation on the benchmarks of Open
501
LLM Leaderboard
502
Open LLM Leaderboard comprises six benchmarks
503
that cover science questions, commonsense infer-
504
6


--- Page 7 ---
Method
Arc
HellaSwag
MMLU
TruthfulQA
Winogrande
GSM8k
Average
Base Model
73.65
79.32
44.42
42.71
74.59
14.94
54.94
Preferred-SFT
71.79
78.79
44.50
49.13
74.59
16.83
55.94
RLHF-PPO
73.79
78.82
44.04
48.22
74.43
17.51
56.22
HIR
73.39
78.40
44.65
46.00
74.51
16.00
55.39
DPO
72.89
79.67
44.88
50.51
74.82
16.22
56.50
RAHF-Dual
72.29
79.16
46.22
52.14
74.51
15.16
56.58
RAHF-SCIT
74.86
79.78
45.77
52.34
74.27
16.60
57.27
Table 1: Results of different methods on six benchmarks of Open LLM Leaderboard. The leaderboard evaluation
configurations and experimental setups adopted in this study are provided in Appendix B.
Method
AlpacaEval (win %)
Preferred-SFT
73.48
HIR
61.81
RLHF-PPO
44.69
DPO
83.68
RAHF-Dual
86.98
RAHF-SCIT
87.44
Table 2: AlpacaEval results, which is the win rate
against text-davinci-003 judged by GPT-4.
ence, multitasking accuracy, and truthfulness in
505
generating answers. We evaluate the models’ gen-
506
eral capabilities on these tasks.
507
In Table 1, we report the results of RAHF and
508
baseline methods across the six benchmarks from
509
OpenLLM. RAHF-SCIT achieves the best results
510
in three benchmarks and improves the score by
511
2.33 on average, compared to the base model.
512
RAHF-Dual exhibits the best performance on the
513
MMLU benchmark. RAHF-SCIT and RAHF-Dual
514
both significantly improve the accuracy of Truth-
515
fulQA and surpass all baselines. Those experimen-
516
tal results demonstrate the effectiveness of RAHF
517
in enhancing the general capabilities of LLM.
518
4.3.2
Evaluation on AlpacaEval
519
AlpacaEval is an automated evaluation benchmark
520
based on LLMs. It employs GPT-4(OpenAI, 2023)
521
as an annotator to compare the generated content
522
of models on simple instruction-following tasks
523
against reference answers from text-davinci-003.
524
Previous work has shown that using GPT-4 as an
525
annotator correlates highly with assessments from
526
human evaluators(Li et al., 2023a). Therefore, we
527
consider AlpacaEval as an automated approxima-
528
tion of human annotation.
529
Table 2 presents the win rates of responses gen-
530
erated by models trained with different methods
531
over 805 samples, compared to the reference re-
532
sponses from text-davinci-003. Both RAHF-SCIT
533
and RAHF-Dual exhibit higher win rates than the
534
baselines which demonstrates the broad effective-
535
ness of RAHF in aligning with human preferences.
536
Figure 3: Scores of RAHF-SCIT and RAHF-Dual com-
pared to competitive methods in MT-Bench. Detailed
results are provided in Appendix C.
4.3.3
Evaluation on MT-Bench
537
MT-Bench is a collection of challenging questions,
538
consisting of 80 samples, each with two turns. This
539
benchmark also employs GPT-4 as a judge to score
540
the responses of models. For each turn, GPT-4 will
541
assign a score on a scale of 10.
542
Figure 3 shows the performance scores achieved
543
by RAHF and the baseline models on 1-turn ques-
544
tions. RAHF outperformed the baselines across
545
multiple metrics, yielding the highest scores in
546
six out of eight evaluated aspects, as well as ex-
547
hibiting the highest average score. Notably, RAHF
548
demonstrated notably superior performance com-
549
pared to the baselines in reasoning, role-play, and
550
STEM tasks. Additionally, despite not being specif-
551
ically fine-tuned for 2-turn dialogue tasks, RAHF
552
still surpassed all baseline models, suggesting that
553
7


Image Path: e:\Codes\super_agent\files\image_7_1.png
Alt Text: Image on page 7
Description: Description for image_7_1.png
Metadata: {}

--- Page 8 ---
its capacity for multi-turn interactions can be en-
554
hanced solely through alignment with 1-turn ques-
555
tion datasets. Comprehensive results for the 2-turn
556
dialogue tasks are provided in Appendix C for de-
557
tailed comparison.
558
4.4
Human Evaluation
559
For the human evaluation, we assigned evaluators
560
the task of comparing two randomly selected re-
561
sponses and providing judgments on their relative
562
performance, categorizing them with three results:
563
win, lose, or tie.
564
Method
Win
Tie
Lose
RAHF-Dual
HIR
74
21
5
RLHF-PPO
88
9
3
DPO
35
43
22
RAHF-SCIT
HIR
79
19
2
RLHF-PPO
88
11
1
DPO
41
38
21
Table 3: Win rates against baselines judged by Humans.
The data in the table represents the proportion of RAHF
relative to the baseline in terms of win, tie, and lose.
Table 3 presents the comparative results of
565
RAHF against RL-free methods and RLHF in hu-
566
man evaluation. The results suggest that RAHF
567
performs better than those methods in alignment
568
with human preferences. The human evaluation
569
results also agree broadly with the GPT-4 evalua-
570
tion results, with the only difference that humans
571
tend to provide more tie judgments than the GPT-4
572
would.
573
4.5
Ablation Study
574
To evaluate the influence of instructing LLMs
575
on human preferences using a human-annotated
576
dataset, we executed ablation experiments involv-
577
ing the exclusion of this instructional phase. More
578
precisely, we compared RAHF against a baseline
579
model devoid of a dedicated preference learning
580
step, instead relying solely on representation engi-
581
neering as outlined in prior work.
582
LORRA
Low-Rank Representation Adaptation
583
proposed by (Zou et al., 2023) does not leverage
584
additional data to learn human preferences. This
585
baseline omits the step of explicit preference learn-
586
ing and evaluates the model’s performance based
587
on representation engineering alone.
588
LORRA-Pref
LORRA-Pref exclusively utilizes
589
preferred responses from the preference dataset
590
AlpacaEval
MT(Turn-1)
MT(Turn-2)
MT(Final)
0
20
40
60
80
AlpacaEval Score
0
1
2
3
4
5
6
7
MT Scores
Method
LORRA
LORRA-Pref
RAHF-SCIT
RAHF-Dual
Figure 4: Performance comparison between RAHF and
methods solely focused on representation engineering
on AlpacaEval and MT-Bench. Detailed results are
provided in Appendix C.
for representation learning instead of employing
591
contrastive learning methods.
592
This ablation analysis allows us to isolate and
593
quantify the impact of assimilating human prefer-
594
ences into the framework of our proposed approach.
595
The results of the ablation experiments shown in
596
Figure 4 indicate that, in the absence of explicit
597
preference learning steps, the approach of directly
598
extracting activity patterns for comparison demon-
599
strates a decline in performance on AlpacaEval and
600
MT-Bench we assessed.
601
5
Conclusion
602
In this study, we have explored a representation en-
603
gineering approach to aligning large language mod-
604
els with human preferences, drawing upon insights
605
from cognitive neuroscience. We introduced RAHF
606
(representation alignment from human feedback), a
607
straightforward paradigm designed for training lan-
608
guage models to align with human preferences at a
609
lower computational cost, eliminating the need for
610
reinforcement learning and reward models. RAHF
611
can effectively identify disparities in the activity
612
patterns of LLMs caused by preferred and dispre-
613
ferred stimuli, and harness these distinctions to
614
improve the controllability of LLMs. We proposed
615
two different methods to implement RAHF and
616
conducted extensive experiments to validate their
617
effectiveness. We hope this study can inspire fu-
618
ture research toward developing more controllable
619
AI and designing more efficient and scalable algo-
620
rithms that could substantially reduce the costs as-
621
sociated with training LLMs with human feedback
622
through the lens of representation engineering.
623
8


--- Page 9 ---
Limitations
624
In this study, we validated the effectiveness of
625
RAHF on LLMs with 7B parameters. However,
626
given the impact of parameter quantity on model
627
capabilities, exploring the extension of RAHF to
628
state-of-the-art models of even larger magnitudes
629
represents an exciting direction for future work.
630
Additionally, in constructing the final model, the
631
difference vector is fitted by the LoRA matrix. An
632
inherent limitation of this methodology is that it
633
introduces additional parameters, although the ex-
634
tra computational overhead incurred by LoRA is
635
minimal. For future work, it would be preferable to
636
consider directly integrating the difference vector
637
into the original model, which could reduce the
638
cost associated with additional parameters.
639
References
640
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda
641
Askell, Anna Chen, Nova DasSarma, Dawn Drain,
642
Stanislav Fort, Deep Ganguli, Tom Henighan, et al.
643
2022. Training a helpful and harmless assistant with
644
reinforcement learning from human feedback. arXiv
645
preprint arXiv:2204.05862.
646
Edward Beeching,
Clémentine Fourrier,
Nathan
647
Habib, Sheon Han, Nathan Lambert, Nazneen
648
Rajani, Omar Sanseviero, Lewis Tunstall, and
649
Thomas Wolf. 2023.
Open llm leaderboard.
650
https://huggingface.co/spaces/
651
HuggingFaceH4/open_llm_leaderboard.
652
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
653
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
654
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
655
Askell, et al. 2020. Language models are few-shot
656
learners. In Proceedings of the Conference on Neural
657
Information Processing Systems (NeurIPS 2020).
658
Sébastien Bubeck, Varun Chandrasekaran, Ronen El-
659
dan, Johannes Gehrke, Eric Horvitz, Ece Kamar,
660
Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund-
661
berg, et al. 2023.
Sparks of artificial general in-
662
telligence: Early experiments with GPT-4. arXiv
663
preprint arXiv:2303.12712.
664
Hyung Won Chung, Le Hou, Shayne Longpre, Barret
665
Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi
666
Wang, Mostafa Dehghani, Siddhartha Brahma, et al.
667
2022. Scaling instruction-finetuned language models.
668
arXiv preprint arXiv:2210.11416.
669
Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao,
670
Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and
671
Maosong Sun. 2023. Ultrafeedback: Boosting lan-
672
guage models with high-quality feedback.
arXiv
673
preprint arXiv:2310.01377.
674
Vincent Dumoulin, Daniel D Johnson, Pablo Samuel
675
Castro,
Hugo Larochelle,
and Yann Dauphin.
676
2023. A density estimation perspective on learn-
677
ing from pairwise human preferences. arXiv preprint
678
arXiv:2311.14115.
679
Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman,
680
Sid Black, Anthony DiPofi, Charles Foster, Laurence
681
Golding, Jeffrey Hsu, Alain Le Noac’h, Haonan Li,
682
Kyle McDonell, Niklas Muennighoff, Chris Ociepa,
683
Jason Phang, Laria Reynolds, Hailey Schoelkopf,
684
Aviya Skowron, Lintang Sutawika, Eric Tang, An-
685
ish Thite, Ben Wang, Kevin Wang, and Andy Zou.
686
2023. A framework for few-shot language model
687
evaluation.
688
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,
689
Bruna Morrone, Quentin De Laroussilhe, Andrea
690
Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019.
691
Parameter-efficient transfer learning for nlp. In In-
692
ternational Conference on Machine Learning, pages
693
2790–2799. PMLR.
694
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan
695
Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,
696
and Weizhu Chen. 2021.
Lora: Low-rank adap-
697
tation of large language models.
arXiv preprint
698
arXiv:2106.09685.
699
Julia Kreutzer, Joshua Uyheng, and Stefan Riezler. 2018.
700
Reliability and learnability of human bandit feedback
701
for sequence-to-sequence reinforcement learning. In
702
Proceedings of the Annual Meeting of the Association
703
for Computational Linguistics (ACL’18).
704
Brian Lester, Rami Al-Rfou, and Noah Constant. 2021.
705
The power of scale for parameter-efficient prompt
706
tuning. arXiv preprint arXiv:2104.08691.
707
Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori,
708
Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and
709
Tatsunori B Hashimoto. 2023a. Alpacaeval: An auto-
710
matic evaluator of instruction-following models.
711
Ziniu Li, Tian Xu, and Yang Yu. 2023b. Policy op-
712
timization in rlhf: The impact of out-of-preference
713
data. arXiv preprint arXiv:2312.10584.
714
Hao Liu, Carmelo Sferrazza, and Pieter Abbeel. 2023.
715
Languages are rewards: Hindsight finetuning using
716
human feedback. arXiv preprint arXiv:2302.02676.
717
OpenAI. 2023.
GPT-4 technical report.
CoRR,
718
abs/2303.08774.
719
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-
720
roll L. Wainwright, Pamela Mishkin, Chong Zhang,
721
Sandhini Agarwal, Katarina Slama, Alex Ray, John
722
Schulman, Jacob Hilton, Fraser Kelton, Luke Miller,
723
Maddie Simens, Amanda Askell, Peter Welinder,
724
Paul Christiano, Jan Leike, and Ryan Lowe. 2022.
725
Training language models to follow instructions with
726
human feedback. In Proceedings of the Conference
727
on Neural Information Processing Systems (NeurIPS
728
2022).
729
9


--- Page 10 ---
Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya
730
Sutskever, et al. 2018. Improving language under-
731
standing by generative pre-training.
732
Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
733
Dario Amodei, Ilya Sutskever, et al. 2019. Language
734
models are unsupervised multitask learners. OpenAI
735
blog, 1(8):9.
736
Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano
737
Ermon, Christopher D Manning, and Chelsea Finn.
738
2023. Direct preference optimization: Your language
739
model is secretly a reward model. arXiv preprint
740
arXiv:2305.18290.
741
Rajkumar Ramamurthy,
Prithviraj Ammanabrolu,
742
Kianté Brantley, Jack Hessel, Rafet Sifa, Christian
743
Bauckhage, Hannaneh Hajishirzi, and Yejin Choi.
744
2023. Is reinforcement learning (not) for natural
745
language processing?: Benchmarks, baselines, and
746
building blocks for natural language policy optimiza-
747
tion. In Proceedings of the International Conference
748
on Learning Representations (ICLR’23).
749
John Schulman, Filip Wolski, Prafulla Dhariwal,
750
Alec Radford, and Oleg Klimov. 2017.
Proxi-
751
mal policy optimization algorithms. arXiv preprint
752
arXiv:1707.06347.
753
Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao,
754
Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch,
755
Adam R Brown, Adam Santoro, Aditya Gupta,
756
Adrià Garriga-Alonso, et al. 2022.
Beyond the
757
imitation game: Quantifying and extrapolating the
758
capabilities of language models.
arXiv preprint
759
arXiv:2206.04615.
760
Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel
761
Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford,
762
Dario Amodei, and Paul F Christiano. 2020. Learn-
763
ing to summarize with human feedback. In Proceed-
764
ings of the Advances in Neural Information Process-
765
ing Systems (NeurIPS’20).
766
Shang-Yu Su, Xiujun Li, Jianfeng Gao, Jingjing Liu,
767
and Yun-Nung Chen. 2019.
Discriminative deep
768
Dyna-Q: Robust planning for dialogue policy learn-
769
ing.
In Proceedings of the Conference on Em-
770
pirical Methods in Natural Language Processing
771
(EMNLP’18).
772
Ryuichi Takanobu, Hanlin Zhu, and Minlie Huang.
773
2019. Guided dialog policy learning: Reward estima-
774
tion for multi-domain task-oriented dialog. In Pro-
775
ceedings of the Conference on Empirical Methods in
776
Natural Language Processing and the International
777
Joint Conference on Natural Language Processing
778
(EMNLP-IJCNLP’19).
779
Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam
780
Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,
781
Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al.
782
2022. LaMDA: Language models for dialog applica-
783
tions. arXiv preprint arXiv:2201.08239.
784
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
785
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
786
Baptiste Rozière, Naman Goyal, Eric Hambro,
787
Faisal Azhar, et al. 2023. LLaMA: Open and ef-
788
ficient foundation language models. arXiv preprint
789
arXiv:2302.13971.
790
Xiaohua Wang, Yuliang Yan, Longtao Huang, Xiaoqing
791
Zheng, and Xuan-Jing Huang. 2023. Hallucination
792
detection for generative large language models by
793
bayesian sequential estimation. In Proceedings of the
794
2023 Conference on Empirical Methods in Natural
795
Language Processing, pages 15361–15371.
796
Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin
797
Guu, Adams Wei Yu, Brian Lester, Nan Du, An-
798
drew M Dai, and Quoc V Le. 2021. Finetuned lan-
799
guage models are zero-shot learners. arXiv preprint
800
arXiv:2109.01652.
801
Ronald J Williams. 1992. Simple statistical gradient-
802
following algorithms for connectionist reinforcement
803
learning. Machine learning, 8:229–256.
804
Muling Wu, Wenhao Liu, Jianhan Xu, Changze Lv, Zix-
805
uan Ling, Tianlong Li, Longtao Huang, Xiaoqing
806
Zheng, and Xuan-Jing Huang. 2023. Parameter ef-
807
ficient multi-task fine-tuning by learning to transfer
808
token-wise prompts. In Findings of the Association
809
for Computational Linguistics: EMNLP 2023, pages
810
8734–8746.
811
Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang,
812
Songfang Huang, and Fei Huang. 2023.
RRHF:
813
Rank responses to align language models with
814
human feedback without tears.
arXiv preprint
815
arXiv:2304.05302.
816
Tianjun Zhang, Fangchen Liu, Justin Wong, Pieter
817
Abbeel, and Joseph E. Gonzalez. 2023. The wisdom
818
of Hindsight makes language models better instruc-
819
tion followers. In Proceedings of the International
820
Conference on Machine Learning (ICML’23).
821
Zhirui Zhang, Xiujun Li, Jianfeng Gao, and Enhong
822
Chen. 2019.
Budgeted policy learning for task-
823
oriented dialogue systems. In Proceedings of the
824
Annual Meeting of the Association for Computational
825
Linguistics (ACL’19).
826
Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman,
827
Mohammad Saleh, and Peter J Liu. 2023. SLIC-HF:
828
Sequence likelihood calibration with human feed-
829
back. arXiv preprint arXiv:2305.10425.
830
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan
831
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
832
Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023.
833
Judging llm-as-a-judge with mt-bench and chatbot
834
arena. arXiv preprint arXiv:2306.05685.
835
Victor Zhong, Caiming Xiong, and Richard Socher.
836
2017. Seq2SQL: Generating structured queries from
837
natural language using reinforcement learning. arXiv
838
preprint arXiv:1709.00103.
839
10


--- Page 11 ---
Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B
840
Brown, Alec Radford, Dario Amodei, Paul Chris-
841
tiano, and Geoffrey Irving. 2019. Fine-tuning lan-
842
guage models from human preferences.
arXiv
843
preprint arXiv:1909.08593.
844
Andy Zou, Long Phan, Sarah Chen, James Campbell,
845
Phillip Guo, Richard Ren, Alexander Pan, Xuwang
846
Yin, Mantas Mazeika, Ann-Kathrin Dombrowski,
847
et al. 2023.
Representation engineering: A top-
848
down approach to AI transparency. arXiv preprint
849
arXiv:2310.01405.
850
11


--- Page 12 ---
A
Prompts
851
A.1
Preference Instructions
852
Figure 5 presents two instructions used in this study
853
for preferred and dispreferred responses.
Preference Instructions
Preferred instruction:
You are a good, respectful and honest assistant. Your 
responses should be helpful, meaningful, truthful, 
harmless, safe, ethical, socially unbiased, positive, 
interesting, well-founded, agreeable, specific, 
reasonable, and role consistent. If a question does not 
make any sense, or is not factually coherent, explain 
why instead of answering something not correct. 
Please do not share false information under any 
circumstances.
Dispreferred instruction:
You are a bad, disrespectful and dishonest assistant. 
Your responses might be useless, meaningless, 
untruthful, harmful, dangerous, unethical, socially 
biased, negative, boring, fabricated, offensive, vague, 
unreasonable, or role inconsistent. If a question does 
not make any sense, or is not factually coherent, still 
answer the question and share false information when 
necessary.
Figure 5: The preference instructions used in RAHF.
854
B
Implementation Details
855
B.1
Training Setups
856
All baselines and our models were trained using An-
857
thropic’s Helpful and Harmless dataset(Bai et al.,
858
2022) fine-tuned model as the base model. During
859
the supervised training of the base model, we cal-
860
culated the loss for both prompts and responses.
861
Specifically, we performed full parameter fine-
862
tuning for three epochs with a learning rate of
863
2e −5.
864
For training, the data is formatted as follows: Hu-
865
man: {prompt} \n\nAssistant: {response}. For all
866
models trained, we established a maximum query
867
length of 256 and a maximum sentence length of
868
768. We exclude samples from the dataset where
869
queries exceed 256 characters and truncate sen-
870
tences to the maximum sentence length. The Ul-
871
traFeedback dataset has been partitioned into a
872
training set. Further, we split the training set into
873
three distinct parts: the first part is utilized in the
874
first step of RAHF for instructing LLM on human
875
preferences, training the reward model within the
876
RLHF-PPO baseline, and for the training of other
877
baselines. The second part is utilized for the con-
878
struction of the final model in RAHF and running
879
the PPO algorithm.
880
B.2
Evaluation Setups
881
For all methods, we employ greedy decoding dur-
882
ing generation on the benchmarks. To avoid the
883
issue of repetition, we set the repetition penalty to
884
1.2.
885
For the Open LLM Leaderboard, we utilized the
886
Eleuther AI Language Model Evaluation Harness
887
library(Gao et al., 2023) to assess language models
888
trained using different methods. Table 4 provides a
889
detailed description of the leaderboard evaluation
890
configuration and the experimental settings adopted
891
in this study.
892
Datasets
# few-shot
Metric
Arc
25
acc_norm
TruthfulQA
0
mc2
Winogrande
5
acc
GSM8k
5
acc
HellaSwag
10
acc_norm
MMLU
5
acc
Table 4: For each dataset used in the evaluation on the
Open LLM Leaderboard, we detail the quantity of few-
shot samples utilized and the specific metric employed
for evaluation.
For Human Evaluation, we recruited six volun-
893
teers for the assessment, with each evaluator com-
894
paring 100 dialogues. Figure 6 shows a screenshot
895
of the interface used for our evaluation, which all
896
evaluators utilized to rate the data.
897
B.3
Experimental Details
898
In this section, we present the experimental details
899
and hyperparameters of the baselines we compare
900
with and our proposed methods.
901
Preferred-SFT
Table 5 presents the hyperparam-
902
eters that were used in Preferred-SFT.
903
Hyperparameter
Value
Learning Rate
2e −5
Epochs
2
Batch Size
128
Micro Batch Size
2
Optimizer
Adamw
LR Scheduler Type
Cosine
Rarmup Ratio
0.1
Table 5: Hyperparameters used for Preferred-SFT.
RLHF-PPO
During the training of RLHF-PPO,
904
we utilized Microsoft’s DeepSpeed-Chat training
905
framework, making adaptive modifications to the
906
hyperparameters. We performed full-parameter
907
fine-tuning for both the training of the reward
908
model and PPO. Table 6 presents the hyperpa-
909
12


--- Page 13 ---
rameters for reward model training, while Table
910
7 presents the key parameters for PPO.
911
Hyperparameter
Value
Learning Rate
9.65e −6
Epochs
3
Optimizer
Adam
Training Batch Size
32
Weight Decay
0.1
Warmup Steps
0
LR Scheduler Type
cosine
Table 6: Hyperparameters used for the training of re-
ward model.
Hyperparameter
Value
Actor Learning Rate
5e −7
Critic Learning Rate
9e −6
KL Coefficient
0.2
Epochs
2
Optimizer
Adam
Training Batch Size
64
Generation Batch Size
64
Weight Decay
0.1
Warmup Steps
10
LR Scheduler Type
Linear
Clip Reward Value
5
Clip Range
0.2
Clip Range Value
5
Gamma
1
Lam
0.95
Table 7: Hyperparameters used for RLHF-PPO.
HIR
For the HIR baseline, we also conducted
912
full-parameter fine-tuning. Table 8 displays the
913
hyperparameters used for HIR.
914
Hyperparameter
Value
Learning Rate
2e −5
Epochs
2
Batch Size
128
Micro Batch Size
4
KL Coefficient
0.001
Label Smoothing
0.2
Entropy Coefficient
0.001
Table 8: Hyperparameters used for HIR.
DPO
We employed the trl framework from Hug-
915
ging Face to train DPO model. we utilized the
916
preferred model from RAHF-Dual, as the refer-
917
ence model for DPO. We employed LoRA for fine-
918
tuning. The hyperparameters used in the DPO train-
919
ing are detailed in Table 9.
920
RAHF-SCIT
For RAHF-SCIT, we used the
921
same hyperparameters as HIR during the first-step
922
training but omitted the supervised training loss.
923
When constructing the final model, we followed
924
the hyperparameter selection in RepE(Zou et al.,
925
2023). We manipulated layers (10, 20, 2) and set
926
Hyperparameter
Value
Learning Rate
2e −5
Epochs
3
Batch Size
128
Micro Batch Size
2
LoRA Rank
16
LoRA Alpha
16
LoRA Dropout
0.05
Beta
0.1
Warmup Ratio
0.1
Optimizer
Adam
Table 9: Hyperparameters used for DPO.
Hyperparameter
Value
Learning Rate
3e −4
Steps
500
Batch Size
16
Micro Batch Size
4
LoRA Rank
8
LoRA Alpha
16
LoRA Dropout
0.05
Alpha
5
max response length
512
LR Scheduler Type
Constant
Table 10: Hyperparameters used for RAHF-SCIT.
the perturbation coefficient α to 5. The details of
927
the hyperparameters are shown in Table 10.
928
RAHF-Dual
For RAHF-Dual, the hyperparame-
929
ters used for the preferred model and dispreferred
930
model during the first step are the same as those
931
used in the Preferred-SFT. For RAHF-Dual, we
932
only utilize the representations of the first 64 to-
933
kens of the response to train the LoRA matrix. This
934
approach is adopted because the influence of the in-
935
struction diminishes for the later generated portions
936
of the response, leading to a decrease in perfor-
937
mance. The hyperparameters used in RAHF-Dual
938
are shown in Table 11.
939
Hyperparameter
Value
Learning Rate
9e −6
Steps
2500
Batch Size
8
Micro Batch Size
8
LoRA Rank
8
LoRA Alpha
16
LoRA Dropout
0.05
Alpha
5
max response length
64
LR Scheduler Type
Constant
Table 11: Hyperparameters used for RAHF-Dual.
C
Experiment Results of MT-Bench
940
Table 12 presents the detailed results of RAHF,
941
baselines, and the ablation study on MT-Bench.
942
13


--- Page 14 ---
Method
Writing
Roleplay
Reasoning
Math
Coding
Extraction
Stem
Humanities
Average
Turn-1
Preferred-SFT
8.500
6.400
4.100
2.200
2.100
4.400
6.500
7.050
6.013
RLHF-PPO
7.775
6.100
3.800
1.900
2.800
4.000
5.450
5.650
4.681
HIR
8.300
4.450
3.900
1.300
2.200
3.150
6.200
7.800
4.663
DPO
9.600
7.000
5.100
2.300
1.600
5.600
9.100
8.900
6.150
LORRA
6.800
5.700
2.300
1.800
2.300
4.050
5.150
6.700
4.350
LORRA-Pref
8.800
6.950
5.100
1.400
2.100
3.800
7.450
8.000
5.450
RAHF-Dual
9.500
7.630
5.200
3.400
2.600
4.030
8.300
8.900
6.195
RAHF-SCIT
9.150
8.000
3.600
2.400
2.200
4.000
8.700
9.350
5.925
Turn-2
Preferred-SFT
4.900
7.000
2.700
1.100
1.900
2.900
6.400
8.200
4.388
RLHF-PPO
5.500
7.500
4.700
1.500
2.600
4.300
6.600
6.400
4.888
HIR
6.500
5.750
1.869
1.900
2.550
2.500
5.650
8.650
4.421
DPO
6.700
7.600
2.700
1.400
2.300
3.300
8.250
9.400
5.206
LORRA
6.050
6.550
2.000
1.200
2.400
4.550
6.300
6.650
4.463
LORRA-Pref
5.800
7.100
3.200
1.400
1.500
5.500
6.800
8.600
4.988
RAHF-Dual
6.650
7.850
4.200
1.400
2.300
3.500
7.800
8.510
5.276
RAHF-SCIT
5.000
7.300
3.600
1.700
1.700
3.700
8.300
9.400
5.088
Final
Preferred-SFT
6.700
6.700
3.400
1.650
2.000
3.650
6.450
7.625
4.772
RLHF-PPO
6.625
6.800
4.250
1.700
2.700
4.150
6.025
6.025
4.784
HIR
7.400
5.100
2.885
1.600
2.375
2.825
5.925
8.225
4.541
DPO
8.150
7.300
3.900
1.850
1.950
4.450
8.675
9.150
5.678
LORRA
6.425
6.125
2.150
1.500
2.350
4.300
5.725
6.675
4.407
LORRA-Pref
7.300
7.025
4.150
1.400
1.800
4.650
7.125
8.300
5.219
RAHF-Dual
8.075
7.740
4.700
2.400
2.450
3.765
8.050
8.705
5.736
RAHF-SCIT
7.075
7.650
3.600
2.050
1.950
3.850
8.500
9.375
5.506
Table 12: Results of MT-Bench.
14


--- Page 15 ---
D
Qualitative Examples
943
Figure 7 and Figure 8 present qualitative exam-
944
ples of RAHF compared with baselines in dialogue
945
tasks.
946
15


--- Page 16 ---
Figure 6: Screenshots of our evaluation interface for rating dialogue. In each instance, evaluators are prompted to
choose the preferred dialogue.
16


Image Path: e:\Codes\super_agent\files\image_16_1.png
Alt Text: Image on page 16
Description: Description for image_16_1.png
Metadata: {}

--- Page 17 ---
Prompt
GIVE ME AN EXECUTIVE SUMMARY OF LOTTADIGITAL.COM
HIR
1) Introduction to Lottery Digital (Lottadigital): A lotterydigital is a digital platform that allows users to play the lottery online, with an emphasis on providing secure and convenient access to various 
lotto games from around the world. The website offers a wide range of features such as instant win scratch cards, daily draws, jackpot games, and more. Users can also participate in syndicates for 
increased chances of winning big prizes.
2) Key Features of LottoDigital: Some key features include: 
a) Secure payment options - Various methods are available including credit/debit card payments, e-wallets like PayPal or Skrill, bank transfers etc., ensuring safe transactions without any hassles. 
b) Wide selection of international lotteries – Players have access to over 30 different national & international lotteries across Europe, Asia Pacific region, North America among others; giving them plenty 
of choices when it comes time to pick their favorite game(s). 
c) Instant Win Scratch Cards – These offer quick results within minutes after purchase which makes them ideal if you’re looking for some fast action! 
d) Daily Draw Games – Including popular ones like EuroMillions & Powerball where players need only one ticket per entry into each respective drawing event held every day at specific times 
throughout weekdays /weekends respectively . 
e) Jackpots Galore – With huge progressive jackpots up for grabs through these specialized lotteries , there's always something exciting waiting just ahead ! 
f) Syndicate Options Available – Join forces with other members by forming groups called "Syndicates" allowing everyone involved share winnings equally amongst themselves based upon how much 
they contributed towards overall pool size used during particular draw events run periodically throughout yearly calendar cycle . 
RLHF-PPO
1. Overview of LottsDigital.com:
LottsDigital is a digital marketing agency that specializes in helping businesses grow their online presence and increase revenue through various services such as SEO, PPC advertising, social media 
management, content creation, and website design. The company was founded by CEO Michael Lotts in 2015 with the goal of providing high-quality, results-driven digital marketing solutions to clients 
across different industries
DPO
1. Introduction to Lotterdigital (Lottadigital):
* Name: Lottadigital is a leading digital lottery platform that offers innovative and secure solutions for the global gaming industry, with its headquarters in Malta. The company was founded by 
experienced professionals from various industries who share a passion for technology and innovation.
2. Overview of Services Provided by Lottadigital:
* Digital Lottery Platforms: Lottadigital provides customized digital lottery platforms tailored to meet clients' specific needs. These platforms are designed to be user-friendly, secure, and compliant with 
regulatory requirements across different jurisdictions worldwide. They offer features such as instant win games, scratch cards, sports betting integration, and more.
3. Key Features of Lottadigital Solutions:
* Security & Compliance: Lottadigital prioritizes security and complies with international standards like ISO/IEC 27001 Information Security Management System certification, PCI DSS Payment Card 
Industry Data Security Standard, and AML Anti-Money Laundering regulations. Their systems undergo regular audits and penetration testing to ensure maximum protection against cyber threats.
4. Partnerships & Collaborations: Lottadigital has established partnerships with several renowned companies within the iGaming sector, including game developers, payment providers, and marketing 
agencies. This enables them to provide their customers with an extensive range of services and resources while ensuring seamless integrations between all parties involved.
5. Awards & Recognition: In recognition of their outstanding work, Lottadigital has received numerous awards over the years, including "Best Newcomer" at the EGR B2B Awards 2019 and "Innovator of 
the Year" at the SBC Awards 2020.
6. Future Plans & Goals: As part of their growth strategy, Lottadigital plans to expand into new markets globally through strategic acquisitions or organically via product development initiatives aimed 
towards enhancing existing products further while exploring emerging technologies relevant to the online gambling space. Additionally, they intend on strengthening relationships with key stakeholders 
within both public authorities and private organizations alike – ultimately contributing positively towards shaping future trends within this dynamic industry landscape.
RAHF-Dual
1. Introduction to Lotter Digital (Lottadigital):
* Founded in 2016 by a team of experienced professionals with expertise in the lottery industry, including software development and marketing.
* Headquartered in Malta, an established jurisdiction for online gaming companies due to its favorable regulatory environment and tax benefits.
* Operates as a B2B provider offering innovative solutions for the global lottery sector through cutting-edge technology platforms and services.
2. Key Products & Services Offerings:
* Customized white label lotto platform tailored according to client needs, featuring advanced features such as multi-language support, mobile compatibility, and seamless integration capabilities.
* Advanced backend system that streamlines operations like ticket sales, prize payouts, player management, and more.
* A comprehensive suite of marketing tools designed to boost customer acquisition and retention rates via personalization strategies, email campaigns, social media promotions, etc.
* Integration with third-party systems like payment processors, CRMs, and data analytics providers to enhance overall efficiency across all business processes.
3. Market Reach & Partnerships:
* Serves clients from various continents worldwide, primarily focusing on European markets but also expanding into other regions where legal regulations permit it.
* Collaborated with several renowned brands within the iGaming space, leveraging their experience and resources to provide better service offerings while maintaining compliance standards set forth by 
respective authorities.
4. Company Milestones & Achievements:
* Successfully launched multiple projects globally, catering to different types of lottery operators ranging from small startups to large corporations.
* Received numerous awards recognizing excellence in product design or outstanding performance over time periods; these include "Best Newcomer" at ICE Totally Gaming 2017 and "Innovator Award" 
at SBC Awards 2019.
* Continuous investment in R&D activities aimed towards enhancing existing products/services portfolio along with introducing new ones based on emerging trends within this niche segment.
RAHF-SCIT
1) Introduction to Lotteries and Digitalization in the Industry
- Overview of lottery games as a form of gambling, with origins dating back centuries.
- Traditionally, lotto has been played through physical channels like retail stores or government-run outlets.
2) Problem Statement & Opportunity for Innovation
- The traditional way of playing lotteries is becoming obsolete due to technological advancements.
- Players are increasingly turning towards digital platforms that offer convenience, accessibility, and more engaging experiences.
- This shift presents an opportunity for innovative solutions that can leverage technology to enhance the overall experience while maintaining regulatory compliance.
3) Solution Proposed by Lottadigital.com
- A platform designed specifically for online lottery players, offering a seamless user journey from registration to ticket purchase and results checking.
- It provides secure transactions via blockchain technology, ensuring transparency and trustworthiness.
- Users have access to various features such as personalized notifications, social media integration, and rewards programs.
4) Benefits of Using Lottadigital.com
- Convenience – Accessible anytime anywhere on mobile devices or desktop computers.
- Security – Blockchain technology guarantees safe and transparent transactions.
- Engagement – Various interactive elements keep users engaged throughout their entire play experience.
- Rewards Programs – Loyalty points system encourages repeat purchases and promotes customer loyalty.
5) Market Analysis & Competition Landscape
- Global market size of $300 billion (lottery industry).
- Estimated growth rate at ~6% annually.
- High competition among existing providers but room for new entrants who can provide unique value propositions.
6) Target Customer Segments & Key Differentiators
- Younger demographic groups interested in modernizing how they gamble.
- Tech-savvy individuals looking for convenient ways to participate in lotteries.
- Regular lottery players seeking enhanced gameplay experiences.
7) Business Model & Revenue Streams
- Subscription fees charged monthly/annually based on plan type selected.
- Transaction fee collected per transaction made within the app.
- Advertising revenues generated from sponsorships and banner ad placements.
- Potential future monetization opportunities include virtual goods sales, merchandise, etc.
8) Marketing Strategy & Go-To-Market Approach
- Initial focus will be on key markets where there's high demand for digitally enabled lottery services.
- Social media campaigns targeted toward younger audiences.
- Partnerships with influencers and celebrities known for their love of gaming.
- Continuously improve product based on feedback received from customers.
Figure 7: RAHF-Dual and RAHF-SCIT are more comprehensive and insightful compared to HIR, RLHF-PPO,
and DPO. RAHF-Dual provides a detailed breakdown of Lotter Digital’s foundation, key offerings, market reach,
and achievements, showcasing a well-rounded view of the company’s impact and growth in the lottery industry.
RAHF-SCIT emphasizes digital transformation in the lottery sector, highlighting the problem statement, innovative
solutions offered by Lottadigital.com, and the benefits and market potential of these solutions. In contrast, HIR,
RLHF-PPO, and DPO responses either mix up the company’s focus, provide less depth in analysis, or lack specificity
regarding the unique value proposition and technological advancements brought by Lottadigital.com.
17


--- Page 18 ---
Prompt
Create a table with the planets of the solar system and their dimensions
HIR
1. Mercury - Diameter = 4,879 km; Mass = 302 kg/m^3  
2. Venus - Diameter = 6,052 km; Mass = 5.24 g/cm^3  
3. Earth - Diameter = 12,742 km; Mass = 5.51 g/cm^3  
4. Mars - Diameter = 6,794 km; Mass = 3.93 g/cm^3  
5. Jupiter - Diameter = 142,984 km; Mass = 1.327 g/cm^3  
6. Saturn - Diameter = 120,536 km; Mass = 0.954 g/cm^3  
7. Uranus - Diameter = 51,118 km; Mass = 1.27 g/cm^3  
8. Neptune - Diameter = 49,528 km; Mass = 1.645 g/cm^3
RLHF-PPO
1. Planet Name                   | Dimensions
DPO
| Planet                           | Dimensions (km)   |
|-----------------------------|---------------------------|
| Mercury                      | 4,879 km diameter     |
| Venus                        | 12,092 km diameter    |
| Earth                         | 12,756 km diameter    |
| Mars                          | 6.3 million km radius |
| Jupiter                       | 11.2 times larger than Earth's diameter |
| Saturn                        | 10.5 times larger than Earth's diameter |
| Uranus                        | 4 times larger than Neptune's diameter |
| Neptune                       | 4 times larger than Pluto's diameter |
RAHF-Dual
1. Planet Name                 | Dimensions (Approximate)  
-----------------------------------|-----------------------------
Mercury                             | Diameter: ~4,879 km           | Mass: ~302.6 million kg
Venus                               | Diameter: ~12,104 km          | Mass: ~4.85 × 10^24 kg
Earth                                | Diameter: ~12,742 km            | Mass: ~5.97 × 10^24 kg
Mars                                 | Diameter: ~6,794 km              | Mass: ~6.41 × 10^23 kg
Jupiter                              | Diameter: ~142,984 km           | Mass: ~1.90 × 10^27 kg
Saturn                               | Diameter: ~120,536 km           | Mass: ~5.68 × 10^26 kg
Uranus                                | Diameter: ~51,118 km            | Mass: ~8.65 × 10^25 kg
Neptune                               | Diameter: ~49,528 km            | Mass: ~1.02 × 10^26 kg
RAHF-SCIT
Table for Planets in Our Solar System and Their Dimensions
----------------------------------------- ---------------
Planet                       Diameter (km)   Mass (kg)    Orbit Distance from Sun (AU)
Mercury               4,879           3.30 × 10^23    57.9 million km
Venus                12,104          4.86 × 10^24     67.2 million km
Earth               ~12,756         5.97 × 10^24    149.6 million km
Mars                6,371            6.41 × 10^23    227.9 million km
Jupiter             142,984        1.90 × 10^27       778.3 million km
Saturn              120,536        9.50 × 10^26       1.4 billion km
Uranus              51,512        1.70 × 10^26       2.8 billion km
Neptune             49,528        1.02 × 10^26       4.5 billion km
Pluto*             2,370           1.90 × 10^22    7.4 billion km
----------------------------------------- ---------------
* Pluto is no longer considered to be part of our solar system as it orbits beyond 
Neptune's orbit at an average distance of approximately 4.5 billion kilometers from the 
sun. It has been reclassified as a dwarf planet due to its size and location relative to 
other objects in the Kuiper Belt.
Figure 8: RAHF-Dual and RAHF-SCIT provide comprehensive, structured data with clear, consistent formatting,
and include additional relevant details such as mass and orbit distance from the Sun. They present accurate,
quantitative information, making them more informative and easier to understand than the less detailed, inconsistent,
or partially incorrect responses of HIR RLHF-PPO and DPO, which lack completeness and clarity in presenting
planetary dimensions and other critical data.
18


